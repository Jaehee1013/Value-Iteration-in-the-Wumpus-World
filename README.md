# Value-Iteration-in-the-Wumpus-World
In this problem, we focus on applying Markov Decision Processes (MDPs) and the Value Iteration algorithm to the Wumpus World scenario. The objective is to implement two functions, MDP_value_iteration() and MDP_policy(), for a 4x4 grid Wumpus World. The goal is to control the agent within this environment and achieve specific objectives.

There are three settings for each of the following parameter combinations of temperature ğ‘‡, epochs ğ‘’, and decay ratio ğ‘‘ to observe and analyze the outcome over the change in temperature.

To analyze the algorithm's behavior under different settings, the program is ran for each of the specified parameter combinations, then outputs utilities and policy.

Setting I: ğ›¾ = 0.3, ğœ‚ = 0.1, ğ‘’ = 10000

Setting II: ğ›¾ = 0.6, ğœ‚ = 0.1, ğ‘’ = 10000

Setting III: ğ›¾ = 0.9, ğœ‚ = 0.1, ğ‘’ = 10000

The following is the observed utilities and policies for each setting

Setting I: 
Utilities and Policy for the Given Wumpus World:
-0.38 â†’ | 0.23 â†’ | 2.76 â†’ | 13.24 â†’ |

-0.52 â†’ | -0.35 â†’ | 0.32 â†‘ | 2.86 â†‘ |

-0.57 â†‘ | -0.66 â†‘ | -4.95 â†‘ | 0.13 â†‘ |

-0.56 â† | -10.17 â† | -5.16 â†’ | -0.41 â†’ |

Setting II: 
Utilities and Policy for the Given Wumpus World:
1.91 â†’ | 4.55 â†’ | 9.86 â†’ | 20.58 â†’ |

0.78 â†’ | 2.30 â†’ | 5.06 â†‘ | 10.37 â†‘ |

-0.03 â†‘ | 0.53 â†‘ | -2.30 â†‘ | 4.68 â†‘ |

-0.25 â†“ | -9.58 â†“ | -5.05 â†’ | 1.61 â†‘ |

Setting III: 
Utilities and Policy for the Given Wumpus World:
40.16 â†’ | 46.89 â†’ | 55.51 â†’ | 66.63 â†’ |

36.41 â†’ | 42.24 â†’ | 48.96 â†’ | 57.02 â†‘ |

31.90 â†’ | 36.19 â†‘ | 37.77 â†‘ | 48.32 â†‘ |

33.90 â†“ | 24.61 â†“ | 27.96 â†‘ | 40.46 â†‘ |
